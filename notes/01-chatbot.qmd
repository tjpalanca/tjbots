---
title: "Chatbot"
subtitle: "Setting up a chatbot for day-to-day testing"
---

## Motivation

I want to create a test harness that allows me to quickly test our various levers, including:

- LLM model providers (e.g. OpenAI, Anthropic, Google) and models (e.g. GPT-4, Claude 3, Gemini)
- System and user prompts
- Tools (e.g. web search, code execution, memory, etc.)
- Configuration (e.g. chain-of-thought)

The sources of these configurations can come from:

- problems I want to solve
- articles I read about LLMs that give me ideas 

### Couldn't we just use the existing apps?

- Most providers will not be model agnostic, they will only support one provider, especially in relation to these open-weight models. 
- Most providers will charge a margin over the raw API cost.
- We cannot experiment with human-AI interfaces because the interfaces are fixed.
- It's harder to compose more complex model configurations and to add tools via MCP servers creates additional overhead.

## Deployment

### Principles

- Only add complexity when necessary but don't create one-way doors.

### Creating a deployable docker container 

- Use the [docker-in-docker devcontainer feature](https://github.com/devcontainers/features/blob/main/src/docker-in-docker/README.md) to quickly add the ability to build images without allowing access to your own machine's docker host.
- [Use `uvicorn` as the main docker command](https://hosting.analythium.io/containerizing-shiny-for-python-and-shinylive-applications/) 
    - `CMD ["uvicorn", "app:app", "--host", "0.0.0.0", "--port", "8080"]`
- We can build off of the devcontainer to create the production docker image.

### Handling authentication and authorization 

### Deploying the docker container 

- We can [use watchtower](https://containrrr.dev/watchtower/) as a lightweight way of updating the docker images as soon as they are updated in the registry. 
    - This also supports docker compose.
- I think we can initially start with just running the bare container and then we can start using something like shinyproxy if we feel the need to scale. 