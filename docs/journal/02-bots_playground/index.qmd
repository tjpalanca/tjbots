# Bots Playground

## Motivation

I want to create a test harness that allows me to quickly test our various levers, including:

-   LLM model providers (e.g. OpenAI, Anthropic, Google) and models (e.g. GPT-4, Claude 3, Gemini)
-   System and user prompts
-   Tools (e.g. web search, code execution, memory, etc.)
-   Configuration (e.g. chain-of-thought)

The sources of these configurations can come from:

-   problems I want to solve
-   articles I read about LLMs that give me ideas

### Couldn't we just use the existing apps?

-   Most providers will not be model agnostic, they will only support one provider, especially in relation to these open-weight models.
-   Most providers will charge a margin over the raw API cost.
-   We cannot experiment with human-AI interfaces because the interfaces are fixed.
-   It's harder to compose more complex model configurations and to add tools via MCP servers creates additional overhead.

## Design

### User interface

-   Mobile first

## Deployment

### Principles

-   Only add complexity when necessary but don't create one-way doors.
-   Minimise drift between development and production as much as possible.
